# Subtree Keyword Index: `torch/distributed/optim/`

## Scope

This index covers all files within `torch/distributed/optim/` and all its subdirectories (recursively).

## Keywords


### __init__

- [`torch/distributed/optim/__init__.py`](./__init__.py_docs.md)

### _deprecation_warning

- [`torch/distributed/optim/_deprecation_warning.py`](./_deprecation_warning.py_docs.md)

### apply_optimizer_in_backward

- [`torch/distributed/optim/apply_optimizer_in_backward.py`](./apply_optimizer_in_backward.py_docs.md)

### files-.py

- [`torch/distributed/optim/__init__.py`](./__init__.py_docs.md)
- [`torch/distributed/optim/utils.py`](./utils.py_docs.md)
- [`torch/distributed/optim/functional_adadelta.py`](./functional_adadelta.py_docs.md)
- [`torch/distributed/optim/post_localSGD_optimizer.py`](./post_localSGD_optimizer.py_docs.md)
- [`torch/distributed/optim/functional_adamax.py`](./functional_adamax.py_docs.md)
- [`torch/distributed/optim/named_optimizer.py`](./named_optimizer.py_docs.md)
- [`torch/distributed/optim/functional_adagrad.py`](./functional_adagrad.py_docs.md)
- [`torch/distributed/optim/functional_rprop.py`](./functional_rprop.py_docs.md)
- [`torch/distributed/optim/functional_adam.py`](./functional_adam.py_docs.md)
- [`torch/distributed/optim/apply_optimizer_in_backward.py`](./apply_optimizer_in_backward.py_docs.md)
- [`torch/distributed/optim/functional_adamw.py`](./functional_adamw.py_docs.md)
- [`torch/distributed/optim/_deprecation_warning.py`](./_deprecation_warning.py_docs.md)
- [`torch/distributed/optim/optimizer.py`](./optimizer.py_docs.md)
- [`torch/distributed/optim/functional_sgd.py`](./functional_sgd.py_docs.md)
- [`torch/distributed/optim/zero_redundancy_optimizer.py`](./zero_redundancy_optimizer.py_docs.md)
- [`torch/distributed/optim/functional_rmsprop.py`](./functional_rmsprop.py_docs.md)

### files-.pyi

- [`torch/distributed/optim/zero_redundancy_optimizer.pyi`](./zero_redundancy_optimizer.pyi_docs.md)

### functional_adadelta

- [`torch/distributed/optim/functional_adadelta.py`](./functional_adadelta.py_docs.md)

### functional_adagrad

- [`torch/distributed/optim/functional_adagrad.py`](./functional_adagrad.py_docs.md)

### functional_adam

- [`torch/distributed/optim/functional_adam.py`](./functional_adam.py_docs.md)

### functional_adamax

- [`torch/distributed/optim/functional_adamax.py`](./functional_adamax.py_docs.md)

### functional_adamw

- [`torch/distributed/optim/functional_adamw.py`](./functional_adamw.py_docs.md)

### functional_rmsprop

- [`torch/distributed/optim/functional_rmsprop.py`](./functional_rmsprop.py_docs.md)

### functional_rprop

- [`torch/distributed/optim/functional_rprop.py`](./functional_rprop.py_docs.md)

### functional_sgd

- [`torch/distributed/optim/functional_sgd.py`](./functional_sgd.py_docs.md)

### named_optimizer

- [`torch/distributed/optim/named_optimizer.py`](./named_optimizer.py_docs.md)

### optimizer

- [`torch/distributed/optim/optimizer.py`](./optimizer.py_docs.md)

### post_localSGD_optimizer

- [`torch/distributed/optim/post_localSGD_optimizer.py`](./post_localSGD_optimizer.py_docs.md)

### utils

- [`torch/distributed/optim/utils.py`](./utils.py_docs.md)

### zero_redundancy_optimizer

- [`torch/distributed/optim/zero_redundancy_optimizer.pyi`](./zero_redundancy_optimizer.pyi_docs.md)
- [`torch/distributed/optim/zero_redundancy_optimizer.py`](./zero_redundancy_optimizer.py_docs.md)


---

*Generated by PyTorch Repository Documentation System*
