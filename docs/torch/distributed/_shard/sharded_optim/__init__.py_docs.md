# Documentation: `torch/distributed/_shard/sharded_optim/__init__.py`

## File Metadata

- **Path**: `torch/distributed/_shard/sharded_optim/__init__.py`
- **Size**: 1,869 bytes (1.83 KB)
- **Type**: Python Source Code
- **Extension**: `.py`

## File Purpose

This is a **Python package initialization file**.

## Original Source

```python
from collections.abc import Iterator
from typing import Union

import torch.nn as nn
from torch.distributed._shard.sharded_tensor import ShardedTensor

from .api import ShardedOptimizer


def named_params_with_sharded_tensor(
    module: nn.Module,
    prefix: str = "",
    recurse: bool = True,
) -> Iterator[tuple[str, Union[nn.Parameter, ShardedTensor]]]:
    r"""Returns an iterator over module parameters (together with the
    ShardedTensor parameters), yielding both the name of the parameter
    as well as the parameter itself. This is typically passed to a
    :class:torch.distributed._shard.sharded_optim.ShardedOptimizer

    Args:
        prefix (str): prefix to prepend to all parameter names.
        recurse (bool): if True, then yields parameters of this module
            and all submodules. Otherwise, yields only parameters that
            are direct members of this module.

    Yields:
        (str, Union[Tensor, ShardedTensor]): Tuple containing
            the name and parameter (or ShardedTensor parameter)

    Example::

        >>> # xdoctest: +SKIP
        >>> model = torch.nn.Linear(*linear_size)
        >>> shard_parameter(model, "weight", spec)
        >>> for name, param in named_params_with_sharded_tensor(model):
        >>>    if name in ['weight']:
        >>>        print(param.size())

    """
    modules = module.named_modules(prefix=prefix) if recurse else [(prefix, module)]

    memo = set()
    for mod_prefix, mod in modules:
        # find all sharded tensor params
        for name, val in vars(mod).items():
            if isinstance(val, ShardedTensor) and val not in memo:
                memo.add(val)
                name = mod_prefix + ("." if mod_prefix else "") + name
                yield name, val

    # find all nn.Parameters
    for name, val in module.named_parameters():
        yield name, val

```



## High-Level Overview

r"""Returns an iterator over module parameters (together with the    ShardedTensor parameters), yielding both the name of the parameter    as well as the parameter itself. This is typically passed to a    :class:torch.distributed._shard.sharded_optim.ShardedOptimizer    Args:        prefix (str): prefix to prepend to all parameter names.        recurse (bool): if True, then yields parameters of this module            and all submodules. Otherwise, yields only parameters that            are direct members of this module.    Yields:        (str, Union[Tensor, ShardedTensor]): Tuple containing            the name and parameter (or ShardedTensor parameter)    Example::        >>> # xdoctest: +SKIP        >>> model = torch.nn.Linear(*linear_size)        >>> shard_parameter(model, "weight", spec)        >>> for name, param in named_params_with_sharded_tensor(model):        >>>    if name in ['weight']:        >>>        print(param.size())

This Python file contains 0 class(es) and 1 function(s).

## Detailed Analysis

### Code Structure

**Functions defined**: `named_params_with_sharded_tensor`

**Key imports**: Iterator, Union, torch.nn as nn, ShardedTensor, ShardedOptimizer


*For complete code details, see the Original Source section above.*


## Architecture & Design

### Role in PyTorch Architecture

This file is located in `torch/distributed/_shard/sharded_optim`, which is part of the **core PyTorch library**.



## Dependencies

### Import Dependencies

This file imports:

- `collections.abc`: Iterator
- `typing`: Union
- `torch.nn as nn`
- `torch.distributed._shard.sharded_tensor`: ShardedTensor
- `.api`: ShardedOptimizer


## Code Patterns & Idioms

### Common Patterns

- **Neural Network**: Defines or uses PyTorch neural network components


## Performance Considerations

### Performance Notes


*Detailed performance analysis requires profiling and benchmarking.*


## Security & Safety

### Security Considerations

- No obvious security concerns detected in automated analysis.

*Manual security review is recommended for production code.*


## Testing & Usage

### Testing

Test files for this module may be located in the `test/` directory.

### Usage Examples

*See the source code and related test files for usage examples.*


## Related Files

### Related Files

Files in the same folder (`torch/distributed/_shard/sharded_optim`):

- [`api.py_docs.md`](./api.py_docs.md)


## Cross-References

- **File Documentation**: `__init__.py_docs.md`
- **Keyword Index**: `__init__.py_kw.md`
- **Folder Index**: `index.md`
- **Folder Documentation**: `doc.md`

---

*Generated by PyTorch Repository Documentation System*
