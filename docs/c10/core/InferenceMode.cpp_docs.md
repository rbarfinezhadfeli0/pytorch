# Documentation: InferenceMode.cpp

## File Metadata
- **Path**: `c10/core/InferenceMode.cpp`
- **Size**: 429 bytes
- **Lines**: 12
- **Extension**: .cpp
- **Type**: Regular file

## Original Source

```cpp
#include <c10/core/InferenceMode.h>

namespace c10 {
// Invariant:
//   is_enabled() ==
//   !c10::impl::tls_is_dispatch_key_included(DispatchKey::ADInplaceOrView);
// InferenceMode::is_enabled() is in perf critical path (TensorImpl constructor)
// so it worths a separate TLS to skip the DispatchKeySet check.
bool InferenceMode::is_enabled() {
  return AutogradState::get_tls_state().get_inference_mode();
}
} // namespace c10

```

## High-Level Overview

This file is part of the PyTorch repository. It is a C++/CUDA source/header file that may contain implementations, declarations, or kernel code.

## Detailed Walkthrough


## Key Components

The file contains 43 words across 12 lines of code/text.

## Usage & Examples

This file is part of the larger PyTorch codebase. For usage examples, refer to related test files and documentation.

## Performance & Security Notes

- File size: 429 bytes
- Complexity: Standard

## Related Files

See the folder index for related files in the same directory.

## Testing

Refer to the PyTorch test suite for test coverage of this file.

---
*Generated by Repo Book Generator v1.0*
