# Keywords: attention.py

## Keyword Index

### A

- **algorithmic**: Identifier found in `attention.py`
- **att_keys**: Identifier found in `attention.py`
- **att_query**: Identifier found in `attention.py`
- **attention**: Identifier found in `attention.py`

### B

- **Bahdanau**: Identifier found in `attention.py`
- **BahdanauAttention**: Identifier found in `attention.py`
- **Benchmark**: Identifier found in `attention.py`
- **b**: Identifier found in `attention.py`
- **benchmark**: Identifier found in `attention.py`
- **benchmarking**: Identifier found in `attention.py`
- **blob**: Identifier found in `attention.py`

### C

- **Calculate**: Identifier found in `attention.py`
- **class**: Identifier found in `attention.py`
- **common**: Identifier found in `attention.py`
- **config**: Identifier found in `attention.py`
- **control**: Identifier found in `attention.py`
- **copy**: Identifier found in `attention.py`

### D

- **default_configs**: Identifier found in `attention.py`
- **device**: Identifier found in `attention.py`
- **dtype**: Identifier found in `attention.py`

### E

- **element_size**: Identifier found in `attention.py`
- **expand**: Identifier found in `attention.py`

### F

- **flow**: Identifier found in `attention.py`
- **forward**: Identifier found in `attention.py`
- **from**: Identifier found in `attention.py`
- **fused**: Identifier found in `attention.py`

### G

- **github**: Identifier found in `attention.py`
- **gnmt**: Identifier found in `attention.py`

### H

- **hardcoded**: Identifier found in `attention.py`
- **https**: Identifier found in `attention.py`

### I

- **import**: Identifier found in `attention.py`
- **input_size**: Identifier found in `attention.py`
- **inputs**: Identifier found in `attention.py`
- **intermediate_size**: Identifier found in `attention.py`
- **io_size**: Identifier found in `attention.py`

### L

- **linear_att**: Identifier found in `attention.py`

### M

- **MLPerf**: Identifier found in `attention.py`
- **master**: Identifier found in `attention.py`
- **matmul**: Identifier found in `attention.py`
- **memory_workload**: Identifier found in `attention.py`
- **memsize**: Identifier found in `attention.py`
- **mlcommons**: Identifier found in `attention.py`
- **mlperf_inference**: Identifier found in `attention.py`
- **mode**: Identifier found in `attention.py`
- **models**: Identifier found in `attention.py`
- **module**: Identifier found in `attention.py`
- **must**: Identifier found in `attention.py`

### N

- **n**: Identifier found in `attention.py`
- **normalize_bias**: Identifier found in `attention.py`
- **numel**: Identifier found in `attention.py`
- **numpy**: Identifier found in `attention.py`
- **nvidia**: Identifier found in `attention.py`

### O

- **out**: Identifier found in `attention.py`
- **output_size**: Identifier found in `attention.py`

### P

- **param**: Identifier found in `attention.py`
- **pytorch**: Identifier found in `attention.py`

### R

- **rand**: Identifier found in `attention.py`
- **read**: Identifier found in `attention.py`
- **reference**: Identifier found in `attention.py`
- **register_benchmark_class**: Identifier found in `attention.py`
- **requires_grad**: Identifier found in `attention.py`
- **retired_benchmarks**: Identifier found in `attention.py`
- **return**: Identifier found in `attention.py`
- **rnn_attention**: Identifier found in `attention.py`

### S

- **Size**: Identifier found in `attention.py`
- **score**: Identifier found in `attention.py`
- **scores**: Identifier found in `attention.py`
- **self**: Identifier found in `attention.py`
- **seq2seq**: Identifier found in `attention.py`
- **size**: Identifier found in `attention.py`
- **sizes**: Identifier found in `attention.py`
- **some**: Identifier found in `attention.py`
- **staticmethod**: Identifier found in `attention.py`
- **stripped**: Identifier found in `attention.py`
- **sum_qk**: Identifier found in `attention.py`
- **super**: Identifier found in `attention.py`

### T

- **This**: Identifier found in `attention.py`
- **t_k**: Identifier found in `attention.py`
- **t_q**: Identifier found in `attention.py`
- **tanh**: Identifier found in `attention.py`
- **then**: Identifier found in `attention.py`
- **torch**: Identifier found in `attention.py`
- **training**: Identifier found in `attention.py`

### U

- **unsqueeze**: Identifier found in `attention.py`

### W

- **with**: Identifier found in `attention.py`
- **write**: Identifier found in `attention.py`

### _

- **__init__**: Identifier found in `attention.py`

