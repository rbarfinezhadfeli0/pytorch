# Documentation: FusedAdamWAmsgradKernelImpl.mm

## File Metadata
- **Path**: `aten/src/ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.mm`
- **Size**: 3789 bytes
- **Lines**: 74
- **Extension**: .mm
- **Type**: Regular file

## Original Source

```mm
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#include <ATen/native/mps/operations/FusedAdamWAmsgradKernelImpl.h>

#include <ATen/Dispatch.h>
#include <ATen/native/ForeachUtils.h>
#include <ATen/native/mps/operations/MultiTensorApply.h>
#include <vector>

namespace at::native::mps {

void _fused_adamw_amsgrad_mps_impl_(TensorList params,
                                    TensorList grads,
                                    TensorList exp_avgs,
                                    TensorList exp_avg_sqs,
                                    TensorList max_exp_avg_sqs,
                                    TensorList state_steps,
                                    const double lr,
                                    const double beta1,
                                    const double beta2,
                                    const double weight_decay,
                                    const double eps,
                                    const bool maximize,
                                    const std::optional<Tensor>& grad_scale,
                                    const std::optional<Tensor>& found_inf) {
  std::vector<std::vector<Tensor>> tensor_lists{
      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec(), max_exp_avg_sqs.vec()};

  const auto kernel_name =
      "fused_adamw_amsgrad_" + scalarToMetalTypeString(params[0]) + "_" + scalarToMetalTypeString(state_steps[0]);

  multi_tensor_apply_for_fused_optimizer<5, 512>(kernel_name,
                                                 tensor_lists,
                                                 state_steps,
                                                 FusedAdamEncodingFunctor(),
                                                 lr,
                                                 beta1,
                                                 beta2,
                                                 weight_decay,
                                                 eps,
                                                 maximize);
}
void _fused_adamw_amsgrad_mps_impl_(TensorList params,
                                    TensorList grads,
                                    TensorList exp_avgs,
                                    TensorList exp_avg_sqs,
                                    TensorList max_exp_avg_sqs,
                                    TensorList state_steps,
                                    const Tensor& lr,
                                    const double beta1,
                                    const double beta2,
                                    const double weight_decay,
                                    const double eps,
                                    const bool maximize,
                                    const std::optional<Tensor>& grad_scale,
                                    const std::optional<Tensor>& found_inf) {
  std::vector<std::vector<Tensor>> tensor_lists{
      params.vec(), grads.vec(), exp_avgs.vec(), exp_avg_sqs.vec(), max_exp_avg_sqs.vec()};

  const auto kernel_name =
      "fused_adamw_amsgrad_" + scalarToMetalTypeString(params[0]) + "_" + scalarToMetalTypeString(state_steps[0]);

  multi_tensor_apply_for_fused_optimizer<5, 512>(kernel_name,
                                                 tensor_lists,
                                                 state_steps,
                                                 FusedAdamEncodingFunctor(),
                                                 lr,
                                                 beta1,
                                                 beta2,
                                                 weight_decay,
                                                 eps,
                                                 maximize);
}

} // namespace at::native::mps

```

## High-Level Overview

This file is part of the PyTorch repository. It is a source or configuration file.

## Detailed Walkthrough


## Key Components

The file contains 155 words across 74 lines of code/text.

## Usage & Examples

This file is part of the larger PyTorch codebase. For usage examples, refer to related test files and documentation.

## Performance & Security Notes

- File size: 3789 bytes
- Complexity: Standard

## Related Files

See the folder index for related files in the same directory.

## Testing

Refer to the PyTorch test suite for test coverage of this file.

---
*Generated by Repo Book Generator v1.0*
