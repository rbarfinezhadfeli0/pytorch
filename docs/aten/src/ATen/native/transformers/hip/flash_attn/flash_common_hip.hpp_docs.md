# Documentation: `aten/src/ATen/native/transformers/hip/flash_attn/flash_common_hip.hpp`

## File Metadata

- **Path**: `aten/src/ATen/native/transformers/hip/flash_attn/flash_common_hip.hpp`
- **Size**: 1,854 bytes (1.81 KB)
- **Type**: C++ Header File
- **Extension**: `.hpp`

## File Purpose

This is a c++ header file that is part of the PyTorch project.

## Original Source

```cpp
/******************************************************************************
 * Copyright (c) 2024, Tri Dao.
 ******************************************************************************/

#pragma once

#include <ATen/TensorIndexing.h>
#include <ATen/core/Tensor.h>
#include <ATen/hip/HIPContext.h>
#include <ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h>
#include <ATen/hip/HIPGraphsUtils.cuh>

#ifndef AT_PER_OPERATOR_HEADERS
#include <ATen/Functions.h>
#include <ATen/NativeFunctions.h>
#else
#include <ATen/ops/empty.h>
#include <ATen/ops/empty_like.h>
#include <ATen/ops/narrow.h>
#include <ATen/ops/pad.h>
#include <ATen/ops/reshape.h>
#include <ATen/ops/scalar_tensor.h>
#include <ATen/ops/sum.h>
#include <ATen/ops/zeros.h>
#endif

#ifdef OLD_GENERATOR_PATH
#include <ATen/CUDAGeneratorImpl.h>
#else
#include <ATen/hip/HIPGeneratorImpl.h>
#endif

#include <ATen/native/transformers/hip/flash_attn/flash_api.h>

#define CHECK_DEVICE(x) TORCH_CHECK(x.is_cuda(), #x " must be on CUDA")
#define CHECK_SHAPE(x, ...)                        \
  TORCH_CHECK(                                     \
      x.sizes() == at::IntArrayRef({__VA_ARGS__}), \
      #x " must have shape (" #__VA_ARGS__ ")")
#define CHECK_CONTIGUOUS(x) \
  TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")

namespace flash {
inline __global__ void ParsePhiloxCudaState(
    at::PhiloxCudaState arg,
    uint64_t* rng_state) {
  // Imitate from PyTorch
  // https://github.com/pytorch/pytorch/blob/8b61daaf7349e9102117e1aeefaa51666d887547/aten/src/ATen/cuda/detail/UnpackRaw.cuh#L17
  if (arg.captured_) {
    rng_state[0] = static_cast<uint64_t>(*arg.seed_.ptr);
    rng_state[1] =
        static_cast<uint64_t>(*(arg.offset_.ptr) + arg.offset_intragraph_);
  } else {
    rng_state[0] = arg.seed_.val;
    rng_state[1] = arg.offset_.val;
  }
}

} // namespace flash

```



## High-Level Overview


This C++ file contains approximately 0 class(es)/struct(s) and 6 function(s).

## Detailed Analysis

### Code Structure

**Namespaces**: `flash`


*For complete code details, see the Original Source section above.*


## Architecture & Design

### Role in PyTorch Architecture

This file is located in `aten/src/ATen/native/transformers/hip/flash_attn`, which is part of **ATen** (A Tensor Library), PyTorch's C++ tensor library.



## Dependencies

### Import Dependencies

This file includes:

- `ATen/TensorIndexing.h`
- `ATen/core/Tensor.h`
- `ATen/hip/HIPContext.h`
- `ATen/hip/impl/HIPGuardImplMasqueradingAsCUDA.h`
- `ATen/hip/HIPGraphsUtils.cuh`
- `ATen/Functions.h`
- `ATen/NativeFunctions.h`
- `ATen/ops/empty.h`
- `ATen/ops/empty_like.h`
- `ATen/ops/narrow.h`
- `ATen/ops/pad.h`
- `ATen/ops/reshape.h`
- `ATen/ops/scalar_tensor.h`
- `ATen/ops/sum.h`
- `ATen/ops/zeros.h`
- `ATen/CUDAGeneratorImpl.h`
- `ATen/hip/HIPGeneratorImpl.h`
- `ATen/native/transformers/hip/flash_attn/flash_api.h`


## Code Patterns & Idioms

### Common Patterns

*No specific patterns automatically detected.*


## Performance Considerations

### Performance Notes

- This file appears to involve **GPU/parallel computing** capabilities.

*Detailed performance analysis requires profiling and benchmarking.*


## Security & Safety

### Security Considerations

- No obvious security concerns detected in automated analysis.

*Manual security review is recommended for production code.*


## Testing & Usage

### Testing

Test files for this module may be located in the `test/` directory.

### Usage Examples

*See the source code and related test files for usage examples.*


## Related Files

### Related Files

Files in the same folder (`aten/src/ATen/native/transformers/hip/flash_attn`):

- [`flash_api.h_docs.md`](./flash_api.h_docs.md)


## Cross-References

- **File Documentation**: `flash_common_hip.hpp_docs.md`
- **Keyword Index**: `flash_common_hip.hpp_kw.md`
- **Folder Index**: `index.md`
- **Folder Documentation**: `doc.md`

---

*Generated by PyTorch Repository Documentation System*
