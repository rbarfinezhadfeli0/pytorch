# Documentation: `aten/src/ATen/native/transformers/hip/flash_attn/ck/me_ck_api.h`

## File Metadata

- **Path**: `aten/src/ATen/native/transformers/hip/flash_attn/ck/me_ck_api.h`
- **Size**: 1,767 bytes (1.73 KB)
- **Type**: C/C++ Header File
- **Extension**: `.h`

## File Purpose

This is a c/c++ header file that is part of the PyTorch project.

## Original Source

```c
#pragma once
#include <cstddef>

#include <ATen/core/Tensor.h>

#if defined(USE_ROCM_CK_SDPA)
namespace pytorch_flash {

std::tuple<
    at::Tensor, // output
    at::Tensor, // q
    at::Tensor, // k
    at::Tensor, // v
    at::Tensor, // lse
    at::Tensor, // seed
    at::Tensor, // offset
    at::Tensor> // dropout randval
mem_eff_forward_ck(
    const at::Tensor& q,
    const at::Tensor& k,
    const at::Tensor& v,
    float p_dropout,
    bool return_dropout_randval,
    std::optional<bool> is_causal,
    std::optional<float> scale,
    const std::optional<at::Tensor>& attn_bias_,
    std::optional<at::Tensor>& out_,
    const std::optional<at::Tensor>& cu_seqlens_q,
    const std::optional<at::Tensor>& cu_seqlens_k,
    const std::optional<at::Tensor>& seqstart_q,
    const std::optional<at::Tensor>& seqstart_k,
    std::optional<at::Generator> gen_,
    std::optional<at::Tensor>& seqused_k_
);

std::tuple<
    at::Tensor, // dQ
    at::Tensor, // dK
    at::Tensor, // dV
    at::Tensor> // dBias
mem_eff_backward_ck(
    const at::Tensor &dout,
    const at::Tensor &q,
    const at::Tensor &k,
    const at::Tensor &v,
    const at::Tensor &out,
    const at::Tensor &softmax_lse,
    const at::Tensor &dq_,
    const at::Tensor &dk_,
    const at::Tensor &dv_,
    std::optional<at::Tensor> &attn_bias,
    bool bias_requires_grad,
    std::optional<at::Tensor> &grad_bias,
    std::optional<at::Tensor> &cu_seqlens_q,
    std::optional<at::Tensor> &cu_seqlens_k,
    int max_seqlen_q,
    int max_seqlen_k,
    float p_dropout,
    float scale,
    bool is_causal,
    bool deterministic,
    bool zero_tensors,
    const at::Tensor philox_seed,
    const at::Tensor philox_offset);

} // namespace pytorch_flash
#endif // USE_ROCM_CK_SDPA

```



## High-Level Overview


This C++ file contains approximately 0 class(es)/struct(s) and 3 function(s).

## Detailed Analysis

### Code Structure

**Namespaces**: `pytorch_flash`


*For complete code details, see the Original Source section above.*


## Architecture & Design

### Role in PyTorch Architecture

This file is located in `aten/src/ATen/native/transformers/hip/flash_attn/ck`, which is part of **ATen** (A Tensor Library), PyTorch's C++ tensor library.



## Dependencies

### Import Dependencies

This file includes:

- `cstddef`
- `ATen/core/Tensor.h`


## Code Patterns & Idioms

### Common Patterns

*No specific patterns automatically detected.*


## Performance Considerations

### Performance Notes


*Detailed performance analysis requires profiling and benchmarking.*


## Security & Safety

### Security Considerations

- No obvious security concerns detected in automated analysis.

*Manual security review is recommended for production code.*


## Testing & Usage

### Testing

Test files for this module may be located in the `test/` directory.

### Usage Examples

*See the source code and related test files for usage examples.*


## Related Files

### Related Files

Files in the same folder (`aten/src/ATen/native/transformers/hip/flash_attn/ck`):

- [`add_make_kernel_pt.sh_docs.md`](./add_make_kernel_pt.sh_docs.md)
- [`launch_kernel_pt.hpp_docs.md`](./launch_kernel_pt.hpp_docs.md)
- [`CMakeLists.txt_docs.md`](./CMakeLists.txt_docs.md)


## Cross-References

- **File Documentation**: `me_ck_api.h_docs.md`
- **Keyword Index**: `me_ck_api.h_kw.md`
- **Folder Index**: `index.md`
- **Folder Documentation**: `doc.md`

---

*Generated by PyTorch Repository Documentation System*
