# Documentation: sdp_utils.h

## File Metadata
- **Path**: `aten/src/ATen/native/transformers/cuda/sdp_utils.h`
- **Size**: 627 bytes
- **Lines**: 17
- **Extension**: .h
- **Type**: Regular file

## Original Source

```h
#pragma once

#include <ATen/Context.h>
#include <c10/macros/Macros.h>
#include <ATen/native/transformers/sdp_utils_cpp.h>
#include <c10/macros/Export.h>

namespace sdp {

bool check_for_seq_len_1_nested_tensor(sdp_params const& params, bool debug);
SDPBackend select_sdp_backend(sdp_params const& kernel_params);
C10_EXPORT bool is_flash_attention_available();
C10_EXPORT bool can_use_flash_attention(sdp_params const& params, bool debug);
C10_EXPORT bool can_use_mem_efficient_attention(sdp_params const& params, bool debug);
C10_EXPORT bool can_use_cudnn_attention(sdp_params const& params, bool debug);

} // namespace sdp

```

## High-Level Overview

This file is part of the PyTorch repository. It is a C++/CUDA source/header file that may contain implementations, declarations, or kernel code.

## Detailed Walkthrough


## Key Components

The file contains 51 words across 17 lines of code/text.

## Usage & Examples

This file is part of the larger PyTorch codebase. For usage examples, refer to related test files and documentation.

## Performance & Security Notes

- File size: 627 bytes
- Complexity: Standard

## Related Files

See the folder index for related files in the same directory.

## Testing

Refer to the PyTorch test suite for test coverage of this file.

---
*Generated by Repo Book Generator v1.0*
