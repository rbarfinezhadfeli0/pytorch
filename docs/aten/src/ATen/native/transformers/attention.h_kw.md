# Keywords: attention.h

## Keyword Index

### A

- **ATen**: Identifier found in `attention.h`
- **attention**: Identifier found in `attention.h`
- **attn_mask**: Identifier found in `attention.h`
- **attn_mask_**: Identifier found in `attention.h`
- **attn_scores**: Identifier found in `attention.h`

### B

- **bmm_nn**: Identifier found in `attention.h`
- **bmm_nt**: Identifier found in `attention.h`
- **bool**: Identifier found in `attention.h`

### C

- **const**: Identifier found in `attention.h`
- **core**: Identifier found in `attention.h`

### D

- **DECLARE_DISPATCH**: Identifier found in `attention.h`
- **DispatchStub**: Identifier found in `attention.h`
- **debug_assert_shape**: Identifier found in `attention.h`
- **double**: Identifier found in `attention.h`
- **dropout_p**: Identifier found in `attention.h`

### E

- **Export**: Identifier found in `attention.h`
- **embed_dim**: Identifier found in `attention.h`
- **enable_gqa**: Identifier found in `attention.h`

### F

- **flash_attention_backward_fn**: Identifier found in `attention.h`
- **flash_attention_backward_kernel**: Identifier found in `attention.h`
- **flash_attention_fn**: Identifier found in `attention.h`
- **flash_attention_kernel**: Identifier found in `attention.h`
- **fused_sdp_choice_fn**: Identifier found in `attention.h`

### G

- **grad_k**: Identifier found in `attention.h`
- **grad_out**: Identifier found in `attention.h`
- **grad_q**: Identifier found in `attention.h`
- **grad_v**: Identifier found in `attention.h`

### I

- **IntArrayRef**: Identifier found in `attention.h`
- **include**: Identifier found in `attention.h`
- **int64_t**: Identifier found in `attention.h`
- **is_causal**: Identifier found in `attention.h`

### L

- **line**: Identifier found in `attention.h`
- **logsumexp**: Identifier found in `attention.h`

### M

- **macros**: Identifier found in `attention.h`
- **mask_type**: Identifier found in `attention.h`
- **masked_softmax**: Identifier found in `attention.h`

### N

- **namespace**: Identifier found in `attention.h`
- **native**: Identifier found in `attention.h`
- **num_head**: Identifier found in `attention.h`

### O

- **once**: Identifier found in `attention.h`
- **optional**: Identifier found in `attention.h`
- **output**: Identifier found in `attention.h`

### P

- **pragma**: Identifier found in `attention.h`

### Q

- **qkv_projection**: Identifier found in `attention.h`
- **qkv_weight**: Identifier found in `attention.h`
- **query**: Identifier found in `attention.h`
- **query_**: Identifier found in `attention.h`

### S

- **ScalarType**: Identifier found in `attention.h`
- **scale**: Identifier found in `attention.h`
- **shape**: Identifier found in `attention.h`

### T

- **TORCH_API**: Identifier found in `attention.h`
- **Tensor**: Identifier found in `attention.h`
- **transform0213_gemm_nt_bias**: Identifier found in `attention.h`
- **transform_bias_rescale_qkv_fn**: Identifier found in `attention.h`
- **transform_bias_rescale_qkv_stub**: Identifier found in `attention.h`
- **transformers**: Identifier found in `attention.h`
- **type**: Identifier found in `attention.h`

### U

- **using**: Identifier found in `attention.h`

### V

- **value**: Identifier found in `attention.h`
- **void**: Identifier found in `attention.h`

