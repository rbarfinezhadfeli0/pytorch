# Documentation: `docs/docs/source/conf.py_docs.md`

## File Metadata

- **Path**: `docs/docs/source/conf.py_docs.md`
- **Size**: 53,355 bytes (52.10 KB)
- **Type**: Markdown Documentation
- **Extension**: `.md`

## File Purpose

This file is part of the **documentation**.

## Original Source

```markdown
# Documentation: `docs/source/conf.py`

## File Metadata

- **Path**: `docs/source/conf.py`
- **Size**: 96,834 bytes (94.56 KB)
- **Type**: Python Source Code
- **Extension**: `.py`

## File Purpose

This file is part of the **documentation**.

## Original Source

```python
#
# PyTorch documentation build configuration file, created by
# sphinx-quickstart on Fri Dec 23 13:31:47 2016.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import inspect
import os

# import sys
import pkgutil
import re
from os import path

# source code directory, relative to this file, for sphinx-autobuild
# sys.path.insert(0, os.path.abspath('../..'))
import torch


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.


try:
    import torchvision  # noqa: F401
except ImportError:
    import warnings

    warnings.warn('unable to load "torchvision" package')

RELEASE = os.environ.get("RELEASE", False)

import pytorch_sphinx_theme2


html_theme = "pytorch_sphinx_theme2"
html_theme_path = [pytorch_sphinx_theme2.get_html_theme_path()]


# -- General configuration ------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.autosummary",
    "sphinx.ext.doctest",
    "sphinx.ext.intersphinx",
    "sphinx.ext.todo",
    "sphinx.ext.coverage",
    "sphinx.ext.napoleon",
    "sphinx.ext.autosectionlabel",
    "sphinxcontrib.katex",
    "sphinx_copybutton",
    "sphinx_design",
    "myst_nb",
    "sphinx.ext.linkcode",
    "sphinxcontrib.mermaid",
    "sphinx_sitemap",
]

myst_enable_extensions = [
    "colon_fence",
    "deflist",
    "html_image",
]

html_baseurl = "https://docs.pytorch.org/docs/stable/"  # needed for sphinx-sitemap
sitemap_locales = [None]
sitemap_excludes = [
    "search.html",
    "genindex.html",
]
sitemap_url_scheme = "{link}"

html_additional_pages = {
    "404": "404.html",
}

# build the templated autosummary files
autosummary_generate = True
numpydoc_show_class_members = False

# autosectionlabel throws warnings if section names are duplicated.
# The following tells autosectionlabel to not throw a warning for
# duplicated section names that are in different documents.
autosectionlabel_prefix_document = True

# katex options
#
#

katex_prerender = True

# General information about the project.
project = "PyTorch"
copyright = "PyTorch Contributors"
author = "PyTorch Contributors"
torch_version = str(torch.__version__)

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
# TODO: change to [:2] at v1.0
version = "main (" + torch_version + " )"
# The full version, including alpha/beta/rc tags.
release = "main"

# Customized html_title here.
# Default is " ".join(project, release, "documentation") if not set
if RELEASE:
    # Turn 1.11.0aHASH into 1.11
    # Note: the release candidates should no longer have the aHASH suffix, but in any
    # case we wish to leave only major.minor, even for rc builds.
    version = ".".join(torch_version.split(".")[:2])
    html_title = " ".join((project, version, "documentation"))
    release = version

switcher_version = "main" if not RELEASE else version

html_static_path = ["_static"]
html_theme_options = {
    "logo": {"text": "Home"},
    "analytics_id": "GTM-T8XT4PS",
    "canonical_url": "https://docs.pytorch.org/docs/stable/",
    "switcher": {
        "json_url": "https://docs.pytorch.org/docs/pytorch-versions.json",
        "version_match": switcher_version,
    },
    "show_toc_level": 2,
    "navigation_with_keys": False,
    "external_links": [
        {
            "name": "Tutorials",
            "url": "https://docs.pytorch.org/tutorials/",
        },
    ],
    "show_version_warning_banner": True,
    "icon_links": [
        {
            "name": "X",
            "url": "https://x.com/PyTorch",
            "icon": "fa-brands fa-x-twitter",
        },
        {
            "name": "GitHub",
            "url": "https://github.com/pytorch/pytorch",
            "icon": "fa-brands fa-github",
        },
        {
            "name": "PyTorch Forum",
            "url": "https://discuss.pytorch.org/",
            "icon": "fa-brands fa-discourse",
        },
        {
            "name": "PyPi",
            "url": "https://pypi.org/project/torch/",
            "icon": "fa-brands fa-python",
        },
    ],
    "navbar_align": "left",
    "navbar_start": ["version-switcher", "navbar-logo"],
    "navbar_center": ["navbar-nav"],
    "navbar_end": ["search-field-custom", "theme-switcher", "navbar-icon-links"],
    "header_links_before_dropdown": 6,
    "navbar_persistent": [],
    "use_edit_page_button": True,
    "pytorch_project": "docs",
}

theme_variables = pytorch_sphinx_theme2.get_theme_variables()
html_context = {
    "github_url": "https://github.com",
    "github_user": "pytorch",
    "github_repo": "pytorch",
    "feedback_url": "https://github.com/pytorch/pytorch",
    "github_version": "main",
    "pytorch_project": "docs",
    "doc_path": "docs/source",
    "theme_variables": theme_variables,
    # library links are defined in
    # pytorch_sphinx_theme2/pytorch_sphinx_theme2/links.json
    "library_links": theme_variables.get("library_links", []),
    "version": version,
    "date_info": {
        "paths_to_skip": ["generated/", "index"],
    },
}

napoleon_use_ivar = True

# Add any paths that contain templates here, relative to this directory.
templates_path = [
    "_templates",
    os.path.join(os.path.dirname(pytorch_sphinx_theme2.__file__), "templates"),
]
# TODO: document these and remove them from here.
# Fixes the duplicated
autosummary_filename_map = {
    "torch.nn.utils.prune.identity": "torch.nn.utils.prune.identity_function",
    "torch.nn.utils.prune.Identity": "torch.nn.utils.prune.Identity_class",
    "torch.optim.adamw.adamw": "torch.optim.adamw.adamw_function",
    "torch.optim.adamw.AdamW": "torch.optim.adamw.AdamW_class",
    "torch.optim.asgd.asgd": "torch.optim.asgd.asgd_function",
    "torch.optim.asgd.ASGD": "torch.optim.asgd.ASGD_class",
    "torch.optim.nadam.nadam": "torch.optim.nadam.nadam_function",
    "torch.optim.nadam.NAdam": "torch.optim.nadam.NAdam_class",
    "torch.optim.radam.radam": "torch.optim.radam.radam_function",
    "torch.optim.radam.RAdam": "torch.optim.radam.RAdam_class",
    "torch.optim.rmsprop.rmsprop": "torch.optim.rmsprop.rmsprop_function",
    "torch.optim.rmsprop.RMSprop": "torch.optim.rmsprop.RMSprop_class",
    "torch.optim.rprop.rprop": "torch.optim.rprop.rprop_function",
    "torch.optim.rprop.Rprop": "torch.optim.rprop.Rprop_class",
    "torch.optim.sgd.sgd": "torch.optim.sgd.sgd_function",
    "torch.optim.sgd.SGD": "torch.optim.sgd.SGD_class",
    "torch.optim.adadelta.adadelta": "torch.optim.adadelta.adadelta_function",
    "torch.optim.adadelta.Adadelta": "torch.optim.adadelta.Adadelta_class",
    "torch.optim.adagrad.adagrad": "torch.optim.adagrad.adagrad_function",
    "torch.optim.adagrad.Adagrad": "torch.optim.adagrad.Adagrad_class",
    "torch.optim.adam.adam": "torch.optim.adam.adam_function",
    "torch.optim.adam.Adam": "torch.optim.adam.Adam_class",
    "torch.optim.adamax.adamax": "torch.optim.adamax.adamax_function",
    "torch.optim.adamax.Adamax": "torch.optim.adamax.Adamax_class",
    "torch.mtia.stream": "torch.mtia.stream_function",
    "torch.mtia.Stream": "torch.mtia.Stream_class",
    "torch.cpu.stream": "torch.cpu.stream_function",
    "torch.cpu.Stream": "torch.cpu.Stream_class",
    "torch.cuda.stream": "torch.cuda.stream_function",
    "torch.cuda.Stream": "torch.cuda.Stream_class",
    "torch.xpu.stream": "torch.xpu.stream_function",
    "torch.xpu.Stream": "torch.xpu.Stream_class",
}

coverage_ignore_functions = [
    # torch
    "typename",
    # torch.cuda._sanitizer
    "zip_arguments",
    "zip_by_key",
    # torch.distributed.autograd
    "is_available",
    # torch.distributed.checkpoint.state_dict
    "gc_context",
    # torch.distributed.elastic.events
    "record_rdzv_event",
    # torch.distributed.elastic.metrics
    "initialize_metrics",
    # torch.distributed.elastic.rendezvous.registry
    "get_rendezvous_handler",
    # torch.distributed.launch
    "launch",
    "main",
    "parse_args",
    # torch.distributed.rpc
    "is_available",
    # torch.distributed.run
    "config_from_args",
    "determine_local_world_size",
    "get_args_parser",
    "get_rdzv_endpoint",
    "get_use_env",
    "main",
    "parse_args",
    "parse_min_max_nnodes",
    "run",
    "run_script_path",
    # torch.distributions.constraints
    "is_dependent",
    # torch.hub
    "import_module",
    # torch.jit
    "export_opnames",
    # torch.jit.unsupported_tensor_ops
    "execWrapper",
    # torch.onnx
    "unregister_custom_op_symbolic",
    # torch.ao.quantization
    "default_eval_fn",
    # torch.backends
    "disable_global_flags",
    "flags_frozen",
    # torch.distributed.algorithms.ddp_comm_hooks
    "register_ddp_comm_hook",
    # torch.nn.parallel
    "DistributedDataParallelCPU",
    # torch.utils
    "set_module",
    "burn_in_info",
    "get_info_and_burn_skeleton",
    "get_inline_skeleton",
    "get_model_info",
    "get_storage_info",
    "hierarchical_pickle",
    # torch.amp.autocast_mode
    "autocast_decorator",
    # torch.ao.nn.quantized.dynamic.modules.rnn
    "apply_permutation",
    "pack_weight_bias",
    # torch.ao.nn.quantized.reference.modules.rnn
    "get_quantized_weight",
    # torch.ao.ns.fx.graph_matcher
    "get_matching_subgraph_pairs",
    # torch.ao.ns.fx.graph_passes
    "add_loggers_to_model",
    "create_a_shadows_b",
    # torch.ao.ns.fx.mappings
    "add_op_to_sets_of_related_ops",
    "get_base_name_for_op",
    "get_base_name_to_sets_of_related_ops",
    "get_node_type_to_io_type_map",
    "get_unmatchable_types_map",
    # torch.ao.ns.fx.n_shadows_utils
    "create_add_loggers_graph",
    "create_n_transformed_and_logged_copies_of_subgraph",
    "create_one_transformed_and_logged_copy_of_subgraph",
    "create_results_comparison",
    "create_submodule_from_subgraph",
    "extract_weight_comparison",
    "group_results_by_subgraph",
    "print_n_shadows_summary",
    # torch.ao.ns.fx.pattern_utils
    "end_node_matches_reversed_fusion",
    "get_reversed_fusions",
    "get_type_a_related_to_b",
    # torch.ao.ns.fx.utils
    "get_arg_indices_of_inputs_to_log",
    "get_node_first_input_and_output_type",
    "get_node_input_qparams",
    "get_normalized_nth_input",
    "get_number_of_non_param_args",
    "get_target_type_str",
    "maybe_add_missing_fqns",
    "maybe_dequantize_first_two_tensor_args_and_handle_tuples",
    "op_type_supports_shadowing",
    "rekey_logger_info_on_node_name_of_model",
    "return_first_non_observer_node",
    # torch.ao.ns.fx.weight_utils
    "extract_weight_from_node",
    "get_conv_fun_weight",
    "get_conv_mod_weight",
    "get_linear_fun_weight",
    "get_linear_mod_weight",
    "get_lstm_mod_weights",
    "get_lstm_weight",
    "get_op_to_type_to_weight_extraction_fn",
    "get_qconv_fun_weight",
    "get_qlinear_fun_weight",
    "get_qlstm_weight",
    "mod_0_weight_detach",
    "mod_weight_bias_0",
    "mod_weight_detach",
    # torch.ao.pruning.sparsifier.utils
    "fqn_to_module",
    "get_arg_info_from_tensor_fqn",
    "module_contains_param",
    "module_to_fqn",
    "swap_module",
    # torch.ao.quantization.backend_config.executorch
    "get_executorch_backend_config",
    # torch.ao.quantization.backend_config.fbgemm
    "get_fbgemm_backend_config",
    # torch.ao.quantization.backend_config.native
    "get_native_backend_config",
    "get_native_backend_config_dict",
    "get_test_only_legacy_native_backend_config",
    "get_test_only_legacy_native_backend_config_dict",
    # torch.ao.quantization.backend_config.onednn
    "get_onednn_backend_config",
    # torch.ao.quantization.backend_config.qnnpack
    "get_qnnpack_backend_config",
    # torch.ao.quantization.backend_config.tensorrt
    "get_tensorrt_backend_config",
    "get_tensorrt_backend_config_dict",
    "get_x86_backend_config",
    # torch.ao.quantization.fuse_modules
    "fuse_known_modules",
    "fuse_modules_qat",
    # torch.ao.quantization.fuser_method_mappings
    "fuse_conv_bn",
    "fuse_conv_bn_relu",
    "fuse_convtranspose_bn",
    "fuse_linear_bn",
    "get_fuser_method",
    "get_fuser_method_new",
    # torch.ao.quantization.fx.convert
    "convert",
    "convert_custom_module",
    "convert_standalone_module",
    "convert_weighted_module",
    # torch.ao.quantization.fx.fuse
    "fuse",
    # torch.ao.quantization.fx.lower_to_fbgemm
    "lower_to_fbgemm",
    # torch.ao.quantization.fx.lower_to_qnnpack
    "lower_to_qnnpack",
    # torch.ao.quantization.fx.pattern_utils
    "get_default_fusion_patterns",
    "get_default_output_activation_post_process_map",
    "get_default_quant_patterns",
    # torch.ao.quantization.fx.prepare
    "insert_observers_for_model",
    "prepare",
    "propagate_dtypes_for_known_nodes",
    "bfs_trace_with_node_process",
    "find_sequential_partitions",
    "get_equivalent_types",
    "update_equivalent_types_dict",
    # torch.ao.quantization.pt2e.prepare
    "prepare",
    # torch.ao.quantization.pt2e.representation.rewrite
    "reference_representation_rewrite",
    # torch.ao.quantization.pt2e.utils
    "fold_bn_weights_into_conv_node",
    "remove_tensor_overload_for_qdq_ops",
    # torch.ao.quantization.qconfig
    "get_default_qat_qconfig",
    "get_default_qat_qconfig_dict",
    "get_default_qconfig",
    "get_default_qconfig_dict",
    "qconfig_equals",
    # torch.ao.quantization.quantization_mappings
    "get_default_dynamic_quant_module_mappings",
    "get_default_dynamic_sparse_quant_module_mappings",
    "get_default_float_to_quantized_operator_mappings",
    "get_default_qat_module_mappings",
    "get_default_qconfig_propagation_list",
    "get_default_static_quant_module_mappings",
    "get_default_static_quant_reference_module_mappings",
    "get_default_static_sparse_quant_module_mappings",
    "get_dynamic_quant_module_class",
    "get_embedding_qat_module_mappings",
    "get_embedding_static_quant_module_mappings",
    "get_quantized_operator",
    "get_static_quant_module_class",
    "no_observer_set",
    # torch.ao.quantization.quantize
    "get_default_custom_config_dict",
    # torch.ao.quantization.quantize_fx
    "attach_preserved_attrs_to_model",
    "convert_to_reference_fx",
    # torch.ao.quantization.quantize_jit
    "convert_dynamic_jit",
    "convert_jit",
    "fuse_conv_bn_jit",
    "prepare_dynamic_jit",
    "prepare_jit",
    "quantize_dynamic_jit",
    "quantize_jit",
    "script_qconfig",
    "script_qconfig_dict",
    # torch.ao.quantization.quantize_pt2e
    "convert_pt2e",
    "prepare_pt2e",
    "prepare_qat_pt2e",
    # torch.ao.quantization.quantizer.embedding_quantizer
    "get_embedding_operators_config",
    # torch.ao.quantization.quantizer.xnnpack_quantizer_utils
    "get_bias_qspec",
    "get_input_act_qspec",
    "get_output_act_qspec",
    "get_weight_qspec",
    "propagate_annotation",
    "register_annotator",
    "activation_dtype",
    "check_node",
    "has_no_children_ignoring_parametrizations",
    "is_per_channel",
    "is_per_tensor",
    "op_is_int8_dynamically_quantized",
    "to_underlying_dtype",
    "weight_dtype",
    "weight_is_quantized",
    "weight_is_statically_quantized",
    # torch.backends.cudnn.rnn
    "get_cudnn_mode",
    "init_dropout_state",
    # torch.backends.xeon.run_cpu
    "create_args",
    # torch.cuda.amp.autocast_mode
    "custom_bwd",
    "custom_fwd",
    # torch.cuda.amp.common
    "amp_definitely_not_available",
    # torch.mtia.memory
    "reset_peak_memory_stats",
    # torch.cuda.nccl
    "all_gather",
    "all_reduce",
    "broadcast",
    "init_rank",
    "reduce",
    "reduce_scatter",
    "unique_id",
    "version",
    # torch.cuda.profiler
    "init",
    "profile",
    "start",
    "stop",
    # torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook
    "hook_with_zero_step",
    "hook_with_zero_step_interleaved",
    # torch.distributed.algorithms.ddp_comm_hooks.post_localSGD_hook
    "post_localSGD_hook",
    # torch.distributed.algorithms.ddp_comm_hooks.quantization_hooks
    "quantization_perchannel_hook",
    "quantization_pertensor_hook",
    # torch.distributed.algorithms.model_averaging.utils
    "average_parameters",
    "average_parameters_or_parameter_groups",
    "get_params_to_average",
    # torch.distributed.checkpoint.default_planner
    "create_default_global_load_plan",
    "create_default_global_save_plan",
    "create_default_local_load_plan",
    "create_default_local_save_plan",
    # torch.distributed.checkpoint.optimizer
    "load_sharded_optimizer_state_dict",
    # torch.distributed.checkpoint.planner_helpers
    "create_read_items_for_chunk_list",
    # torch.distributed.checkpoint.state_dict_loader
    "load_state_dict",
    # torch.distributed.checkpoint.state_dict_saver
    "save_state_dict",
    # torch.distributed.checkpoint.utils
    "find_state_dict_object",
    "find_tensor_shard",
    "isend",
    "monitored_barrier",
    "new_group",
    "new_subgroups",
    "new_subgroups_by_enumeration",
    "recv",
    "reduce",
    "reduce_scatter",
    "reduce_scatter_tensor",
    "scatter",
    "scatter_object_list",
    "send",
    "supports_complex",
    # torch.distributed.elastic.events.handlers
    "get_logging_handler",
    # torch.distributed.elastic.metrics.api
    "configure",
    "getStream",
    "get_elapsed_time_ms",
    "prof",
    "profile",
    "publish_metric",
    "put_metric",
    # torch.distributed.elastic.multiprocessing.api
    "get_std_cm",
    "to_map",
    # torch.distributed.elastic.multiprocessing.errors.handlers
    "get_error_handler",
    # torch.distributed.elastic.multiprocessing.redirects
    "get_libc",
    "redirect",
    # torch.distributed.elastic.multiprocessing.tail_log
    "tail_logfile",
    # torch.distributed.elastic.rendezvous.dynamic_rendezvous
    "get_method_name",
    # torch.distributed.elastic.rendezvous.etcd_rendezvous
    "create_rdzv_handler",
    # torch.distributed.elastic.rendezvous.etcd_server
    "find_free_port",
    "stop_etcd",
    # torch.distributed.elastic.rendezvous.etcd_store
    "cas_delay",
    # torch.distributed.elastic.rendezvous.static_tcp_rendezvous
    "create_rdzv_handler",
    # torch.distributed.elastic.rendezvous.utils
    "parse_rendezvous_endpoint",
    # torch.distributed.elastic.timer.api
    "configure",
    "expires",
    # torch.distributed.elastic.utils.api
    "get_env_variable_or_raise",
    "get_socket_with_port",
    # torch.distributed.elastic.utils.distributed
    "create_c10d_store",
    "get_free_port",
    "get_socket_with_port",
    # torch.distributed.elastic.utils.log_level
    "get_log_level",
    # torch.distributed.elastic.utils.logging
    "get_logger",
    # torch.distributed.elastic.utils.store
    "barrier",
    "get_all",
    "synchronize",
    "store_timeout",
    # torch.distributed.fsdp.wrap
    "always_wrap_policy",
    "enable_wrap",
    "lambda_auto_wrap_policy",
    "size_based_auto_wrap_policy",
    "transformer_auto_wrap_policy",
    "wrap",
    # torch.distributed.nn.functional
    "all_to_all",
    "all_to_all_single",
    # torch.distributed.nn.jit.instantiator
    "get_arg_return_types_from_interface",
    "instantiate_non_scriptable_remote_module_template",
    "instantiate_scriptable_remote_module_template",
    # torch.distributed.nn.jit.templates.remote_module_template
    "get_remote_module_template",
    # torch.distributed.optim.utils
    "as_functional_optim",
    "register_functional_optim",
    # torch.distributed.rendezvous
    "register_rendezvous_handler",
    "rendezvous",
    # torch.distributed.rpc.api
    "get_worker_info",
    "method_factory",
    "new_method",
    "remote",
    "rpc_async",
    "rpc_sync",
    "shutdown",
    # torch.distributed.rpc.backend_registry
    "backend_registered",
    "construct_rpc_backend_options",
    "init_backend",
    "register_backend",
    # torch.distributed.rpc.internal
    "deserialize",
    "serialize",
    # torch.distributed.tensor.parallel.api
    "parallelize_module",
    # torch.distributed.tensor.parallel.input_reshard
    "input_reshard",
    # torch.distributed.tensor.parallel.loss
    "loss_parallel",
    # torch.distributed.tensor.parallel.style
    "make_sharded_output_tensor",
    # torch.distributions.utils
    "broadcast_all",
    "clamp_probs",
    "logits_to_probs",
    "probs_to_logits",
    "tril_matrix_to_vec",
    "vec_to_tril_matrix",
    # torch.fx.annotate
    "annotate",
    # torch.fx.experimental.accelerator_partitioner
    "check_dependency",
    "combine_two_partitions",
    "get_bfs_level_partition",
    "get_device_partition_stats",
    "get_device_to_partitions_mapping",
    "get_logical_id_to_device",
    "get_node_to_partition_mapping",
    "reorganize_partitions",
    "reset_partition_device",
    "set_parents_and_children",
    # torch.fx.experimental.const_fold
    "get_unique_attr_name_in_module",
    "split_const_subgraphs",
    # torch.fx.experimental.debug
    "set_trace",
    # torch.fx.experimental.graph_gradual_typechecker
    "adaptiveavgpool2d_check",
    "adaptiveavgpool2d_inference_rule",
    "add_inference_rule",
    "all_eq",
    "bn2d_inference_rule",
    "broadcast_types",
    "calculate_out_dimension",
    "conv2d_inference_rule",
    "conv_refinement_rule",
    "conv_rule",
    "element_wise_eq",
    "expand_to_tensor_dim",
    "first_two_eq",
    "flatten_check",
    "flatten_inference_rule",
    "flatten_refinement_rule",
    "get_attr_inference_rule",
    "get_greatest_upper_bound",
    "get_parameter",
    "linear_check",
    "linear_inference_rule",
    "linear_refinement_rule",
    "maxpool2d_check",
    "maxpool2d_inference_rule",
    "register_algebraic_expressions_inference_rule",
    "register_inference_rule",
    "register_refinement_rule",
    "relu_inference_rule",
    "reshape_inference_rule",
    "transpose_inference_rule",
    # torch.fx.experimental.merge_matmul
    "are_nodes_independent",
    "may_depend_on",
    "merge_matmul",
    "split_result_tensors",
    # torch.fx.experimental.meta_tracer
    "embedding_override",
    "functional_relu_override",
    "gen_constructor_wrapper",
    "nn_layernorm_override",
    "proxys_to_metas",
    "symbolic_trace",
    "torch_abs_override",
    "torch_nn_relu_override",
    "torch_relu_override",
    "torch_where_override",
    # torch.fx.experimental.migrate_gradual_types.constraint
    "is_algebraic_expression",
    "is_bool_expr",
    "is_dim",
    # torch.fx.experimental.migrate_gradual_types.constraint_generator
    "adaptive_inference_rule",
    "add_layer_norm_constraints",
    "add_linear_constraints",
    "arange_inference_rule",
    "assert_inference_rule",
    "batchnorm_inference_rule",
    "bmm_inference_rule",
    "broadcasting_inference_rule",
    "conv2d_inference_rule",
    "cumsum_inference_rule",
    "embedding_inference_rule",
    "embedding_inference_rule_functional",
    "eq_inference_rule",
    "equality_inference_rule",
    "expand_inference_rule",
    "flatten_inference_rule",
    "full_inference_rule",
    "gen_broadcasting_constraints",
    "gen_embedding_rules",
    "gen_layer_norm_constraints",
    "generate_flatten_constraints",
    "get_attr_inference_rule",
    "getitem_inference_rule",
    "gt_inference_rule",
    "index_select_inference_rule",
    "layer_norm_functional",
    "layer_norm_inference_rule",
    "linear_constraints",
    "linear_inference_rule",
    "lt_inference_rule",
    "masked_fill_inference_rule",
    "maxpool_inference_rule",
    "neq_inference_rule",
    "range_check",
    "register_inference_rule",
    "relu_inference_rule",
    "reshape_inference_rule",
    "size_inference_rule",
    "tensor_inference_rule",
    "torch_dim_inference_rule",
    "torch_linear_inference_rule",
    "transpose_inference_rule",
    "type_inference_rule",
    "view_inference_rule",
    # torch.fx.experimental.migrate_gradual_types.constraint_transformation
    "apply_padding",
    "broadcast_dim",
    "calc_last_two_dims",
    "create_equality_constraints_for_broadcasting",
    "gen_all_reshape_possibilities",
    "gen_broadcasting_constraints",
    "gen_consistency_constraints",
    "gen_greatest_upper_bound",
    "gen_lists_of_dims",
    "generate_all_broadcasting_possibilities_no_padding",
    "generate_all_int_dyn_dim_possibilities",
    "generate_binconstraint_d",
    "generate_binconstraint_t",
    "generate_broadcasting",
    "generate_calc_conv",
    "generate_calc_maxpool",
    "generate_calc_product",
    "generate_conj",
    "generate_d_gub",
    "generate_disj",
    "generate_gub",
    "generate_reshape",
    "is_dim_div_by_target",
    "is_target_div_by_dim",
    "no_broadcast_dim_with_index",
    "register_transformation_rule",
    "transform_constraint",
    "transform_get_item",
    "transform_get_item_tensor",
    "transform_index_select",
    "transform_transpose",
    "valid_index",
    "valid_index_tensor",
    # torch.fx.experimental.migrate_gradual_types.transform_to_z3
    "evaluate_conditional_with_constraints",
    # torch.fx.experimental.migrate_gradual_types.util
    "gen_bvar",
    "gen_dvar",
    "gen_nat_constraints",
    "gen_tensor_dims",
    "gen_tvar",
    # torch.fx.experimental.optimization
    "extract_subgraph",
    "fuse",
    "gen_mkl_autotuner",
    "matches_module_pattern",
    "modules_to_mkldnn",
    "optimize_for_inference",
    "remove_dropout",
    "replace_node_module",
    "reset_modules",
    "use_mkl_length",
    # torch.fx.experimental.partitioner_utils
    "get_comm_latency_between",
    "get_extra_size_of",
    "get_latency_of_one_partition",
    "get_latency_of_partitioned_graph",
    "get_partition_to_latency_mapping",
    "record_shapeenv_event",
    "replay_shape_env_events",
    "shape_env_check_state_equal",
    "sym_sqrt",
    "reify",
    # torch.fx.experimental.unification.match
    "edge",
    "match",
    "ordering",
    "supercedes",
    # torch.fx.experimental.unification.more
    "reify_object",
    "unifiable",
    "unify_object",
    # torch.fx.experimental.unification.multipledispatch.conflict
    "ambiguities",
    "ambiguous",
    "consistent",
    "edge",
    "ordering",
    "super_signature",
    "supercedes",
    # torch.fx.experimental.unification.multipledispatch.core
    "dispatch",
    "ismethod",
    # torch.fx.experimental.unification.multipledispatch.dispatcher
    "ambiguity_warn",
    "halt_ordering",
    "restart_ordering",
    "source",
    "str_signature",
    "variadic_signature_matches",
    "variadic_signature_matches_iter",
    "warning_text",
    # torch.fx.experimental.unification.multipledispatch.utils
    "expand_tuples",
    "groupby",
    "raises",
    "reverse_dict",
    # torch.fx.experimental.unification.multipledispatch.variadic
    "isvariadic",
    "freeze",
    "hashable",
    "raises",
    "reverse_dict",
    "transitive_get",
    "xfail",
    # torch.fx.experimental.unification.variable
    "var",
    "vars",
    # torch.fx.experimental.unify_refinements
    "check_for_type_equality",
    "convert_eq",
    "infer_symbolic_types",
    "infer_symbolic_types_single_pass",
    "substitute_all_types",
    "substitute_solution_one_type",
    "unify_eq",
    # torch.fx.experimental.validator
    "bisect",
    "translation_validation_enabled",
    "translation_validation_timeout",
    "z3op",
    "z3str",
    # torch.fx.graph_module
    "reduce_graph_module",
    "reduce_package_graph_module",
    # torch.fx.node
    "has_side_effect",
    "map_aggregate",
    "map_arg",
    # torch.fx.operator_schemas
    "check_for_mutable_operation",
    "create_type_hint",
    "get_signature_for_torch_op",
    "normalize_function",
    "normalize_module",
    "type_matches",
    # torch.fx.passes.annotate_getitem_nodes
    "annotate_getitem_nodes",
    # torch.fx.passes.backends.cudagraphs
    "partition_cudagraphs",
    # torch.fx.passes.dialect.common.cse_pass
    "get_CSE_banned_ops",
    # torch.fx.passes.graph_manipulation
    "get_size_of_all_nodes",
    "get_size_of_node",
    "get_tensor_meta",
    "replace_target_nodes_with",
    # torch.fx.passes.infra.pass_manager
    "pass_result_wrapper",
    "this_before_that_pass_constraint",
    # torch.fx.passes.operator_support
    "any_chain",
    "chain",
    "create_op_support",
    # torch.fx.passes.param_fetch
    "default_matching",
    "extract_attrs_for_lowering",
    "lift_lowering_attrs_to_nodes",
    # torch.fx.passes.pass_manager
    "inplace_wrapper",
    "log_hook",
    "loop_pass",
    "these_before_those_pass_constraint",
    "this_before_that_pass_constraint",
    # torch.fx.passes.regional_inductor
    "regional_inductor",
    # torch.fx.passes.reinplace
    "reinplace",
    # torch.fx.passes.split_module
    "split_module",
    # torch.fx.passes.split_utils
    "getattr_recursive",
    "setattr_recursive",
    "split_by_tags",
    # torch.fx.passes.splitter_base
    "generate_inputs_for_submodules",
    # torch.fx.passes.tools_common
    "get_acc_ops_name",
    "get_node_target",
    "is_node_output_tensor",
    "legalize_graph",
    # torch.fx.passes.utils.common
    "compare_graphs",
    "lift_subgraph_as_module",
    # torch.fx.passes.utils.fuser_utils
    "erase_nodes",
    "fuse_as_graphmodule",
    "fuse_by_partitions",
    "insert_subgm",
    "topo_sort",
    "validate_partition",
    # torch.fx.passes.utils.source_matcher_utils
    "check_subgraphs_connected",
    "get_source_partitions",
    # torch.fx.proxy
    "assert_fn",
    # torch.fx.subgraph_rewriter
    "replace_pattern",
    "replace_pattern_with_filters",
    # torch.fx.tensor_type
    "is_consistent",
    "is_more_precise",
    # torch.fx.traceback
    "format_stack",
    "get_current_meta",
    "has_preserved_node_meta",
    "preserve_node_meta",
    "reset_grad_fn_seq_nr",
    "set_current_meta",
    "set_grad_fn_seq_nr",
    "set_stack_trace",
    "set_current_replay_node",
    "get_current_replay_node",
    # torch.jit.annotations
    "ann_to_type",
    "check_fn",
    "get_enum_value_type",
    "get_param_names",
    "get_signature",
    "get_type_line",
    "is_function_or_method",
    "is_tensor",
    "is_vararg",
    "parse_type_line",
    "split_type_line",
    "try_ann_to_type",
    "try_real_annotations",
    # torch.jit.frontend
    "build_class_def",
    "build_def",
    "build_ignore_context_manager",
    "build_param",
    "build_param_list",
    "build_stmts",
    "build_withitems",
    "find_before",
    "get_class_assigns",
    "get_class_properties",
    "get_default_args",
    "get_default_args_for_class",
    "get_jit_class_def",
    "get_jit_def",
    "is_reserved_name",
    "is_torch_jit_ignore_context_manager",
    # torch.jit.generate_bytecode
    "format_bytecode",
    "generate_upgraders_bytecode",
    # torch.jit.quantized
    "apply_permutation",
    "quantize_linear_modules",
    "quantize_rnn_cell_modules",
    "quantize_rnn_modules",
    # torch.library
    "define",
    "get_ctx",
    "impl",
    "impl_abstract",
    # torch.masked.maskedtensor.core
    "is_masked_tensor",
    # torch.masked.maskedtensor.creation
    "as_masked_tensor",
    "masked_tensor",
    # torch.multiprocessing.pool
    "clean_worker",
    # torch.multiprocessing.reductions
    "fd_id",
    "init_reductions",
    "rebuild_cuda_tensor",
    "rebuild_meta_tensor",
    "rebuild_event",
    "rebuild_nested_tensor",
    "rebuild_sparse_coo_tensor",
    "rebuild_sparse_compressed_tensor",
    "rebuild_storage_empty",
    "rebuild_storage_fd",
    "rebuild_storage_filename",
    "rebuild_tensor",
    "rebuild_typed_storage",
    "rebuild_typed_storage_child",
    "reduce_event",
    "reduce_storage",
    "reduce_tensor",
    "reduce_typed_storage",
    "reduce_typed_storage_child",
    "storage_from_cache",
    # torch.multiprocessing.spawn
    "start_processes",
    # torch.nn.functional
    "adaptive_max_pool1d_with_indices",  # documented as adaptive_max_pool1d
    "adaptive_max_pool2d_with_indices",  # documented as adaptive_max_pool2d
    "adaptive_max_pool3d_with_indices",  # documented as adaptive_max_pool3d
    "assert_int_or_pair",  # looks unintentionally public
    "fractional_max_pool2d_with_indices",  # documented as fractional_max_pool2d
    "fractional_max_pool3d_with_indices",  # documented as fractional_max_pool3d
    "max_pool1d_with_indices",  # documented as max_pool1d
    "max_pool2d_with_indices",  # documented as max_pool2d
    "max_pool3d_with_indices",  # documented as max_pool3d
    "multi_head_attention_forward",
    # torch.nn.grad
    "conv1d_input",  # legacy helper for gradient computation
    "conv1d_weight",  # legacy helper for gradient computation
    "conv2d_input",  # legacy helper for gradient computation
    "conv2d_weight",  # legacy helper for gradient computation
    "conv3d_input",  # legacy helper for gradient computation
    "conv3d_weight",  # legacy helper for gradient computation
    # torch.nn.init
    "constant",  # deprecated
    "dirac",  # deprecated
    "eye",  # deprecated
    "kaiming_normal",  # deprecated
    "kaiming_uniform",  # deprecated
    "normal",  # deprecated
    "orthogonal",  # deprecated
    "sparse",  # deprecated
    "uniform",  # deprecated
    "xavier_normal",  # deprecated
    "xavier_uniform",  # deprecated
    # torch.nn.modules.rnn
    "apply_permutation",  # deprecated
    # torch.nn.modules.utils
    "consume_prefix_in_state_dict_if_present",
    # torch.nn.parallel.comm
    "broadcast",
    "broadcast_coalesced",
    "gather",
    "reduce_add",
    "reduce_add_coalesced",
    "scatter",
    # torch.nn.parallel.data_parallel
    "data_parallel",
    # torch.nn.parallel.parallel_apply
    "get_a_var",
    "parallel_apply",
    # torch.nn.parallel.replicate
    "replicate",
    # torch.nn.parallel.scatter_gather
    "gather",
    "is_namedtuple",
    "scatter",
    "scatter_kwargs",
    # torch.nn.utils.rnn
    "bind",  # looks unintentionally public
    # torch.onnx.operators
    "reshape_from_tensor_shape",
    "shape_as_tensor",
    # torch.onnx.symbolic_caffe2
    "add",
    "avg_pool2d",
    "cat",
    "conv2d",
    "conv2d_relu",
    "conv_prepack",
    "dequantize",
    "linear",
    "linear_prepack",
    "max_pool2d",
    "nchw2nhwc",
    "nhwc2nchw",
    "quantize_per_tensor",
    "register_quantized_ops",
    "relu",
    "reshape",
    "sigmoid",
    "slice",
    "upsample_nearest2d",
    # torch.onnx.symbolic_helper
    "args_have_same_dtype",
    "check_training_mode",
    "dequantize_helper",
    "is_complex_value",
    "quantize_helper",
    "quantized_args",
    "requantize_bias_helper",
    # torch.onnx.symbolic_opset10
    "dequantize",
    "div",
    "embedding_bag",
    "fake_quantize_per_tensor_affine",
    "flip",
    "fmod",
    "isfinite",
    "isinf",
    "nan_to_num",
    "quantize_per_tensor",
    "quantized_add",
    "quantized_add_relu",
    "quantized_cat",
    "quantized_conv1d",
    "quantized_conv1d_relu",
    "quantized_conv2d",
    "quantized_conv2d_relu",
    "quantized_conv3d",
    "quantized_conv3d_relu",
    "quantized_conv_transpose1d",
    "quantized_conv_transpose2d",
    "quantized_conv_transpose3d",
    "quantized_group_norm",
    "quantized_hardswish",
    "quantized_instance_norm",
    "quantized_layer_norm",
    "quantized_leaky_relu",
    "quantized_linear",
    "quantized_linear_relu",
    "quantized_mul",
    "quantized_sigmoid",
    "slice",
    "sort",
    "topk",
    # torch.onnx.symbolic_opset11
    "Delete",
    "add",
    "append",
    "arange",
    "argsort",
    "atleast_1d",
    "atleast_2d",
    "atleast_3d",
    "cat",
    "chunk",
    "clamp",
    "clamp_max",
    "clamp_min",
    "constant_pad_nd",
    "cumsum",
    "embedding_bag",
    "embedding_renorm",
    "flatten",
    "gather",
    "hardtanh",
    "hstack",
    "im2col",
    "index",
    "index_copy",
    "index_fill",
    "index_put",
    "insert",
    "linalg_det",
    "linalg_vector_norm",
    "logdet",
    "masked_scatter",
    "masked_select",
    "mm",
    "narrow",
    "normal",
    "pad",
    "pixel_shuffle",
    "pop",
    "prim_constant_chunk",
    "reflection_pad",
    "relu6",
    "remainder",
    "replication_pad",
    "round",
    "scatter",
    "select",
    "size",
    "sort",
    "split",
    "split_with_sizes",
    "squeeze",
    "stack",
    "topk",
    "unbind",
    "unique_dim",
    "unsqueeze",
    "vstack",
    # torch.onnx.symbolic_opset12
    "argmax",
    "argmin",
    "binary_cross_entropy_with_logits",
    "celu",
    "cross_entropy_loss",
    "dropout",
    "einsum",
    "ge",
    "le",
    "native_dropout",
    "nll_loss",
    "nll_loss2d",
    "nll_loss_nd",
    "outer",
    "pow",
    "tensordot",
    "unfold",
    # torch.onnx.symbolic_opset13
    "diagonal",
    "fake_quantize_per_channel_affine",
    "fake_quantize_per_tensor_affine",
    "frobenius_norm",
    "log_softmax",
    "nonzero_numpy",
    "quantized_conv1d",
    "quantized_conv1d_relu",
    "quantized_conv2d",
    "quantized_conv2d_relu",
    "quantized_conv3d",
    "quantized_conv3d_relu",
    "quantized_conv_transpose1d",
    "quantized_conv_transpose2d",
    "quantized_conv_transpose3d",
    "quantized_linear",
    "quantized_linear_relu",
    "repeat_interleave",
    "softmax",
    "split",
    "split_with_sizes",
    "tensor_split",
    "tile",
    "unbind",
    "unflatten",
    "unsafe_chunk",
    "unsafe_split",
    "unsafe_split_with_sizes",
    "where",
    # torch.onnx.symbolic_opset14
    "batch_norm",
    "hardswish",
    "quantized_hardswish",
    "reshape",
    "scaled_dot_product_attention",
    "tril",
    "triu",
    # torch.onnx.symbolic_opset15
    "aten__is_",
    "aten__isnot_",
    "bernoulli",
    "prim_unchecked_cast",
    # torch.onnx.symbolic_opset16
    "grid_sampler",
    "scatter_add",
    "scatter_reduce",
    # torch.onnx.symbolic_opset17
    "layer_norm",
    "stft",
    # torch.onnx.symbolic_opset18
    "col2im",
    # torch.onnx.symbolic_opset7
    "max",
    "min",
    # torch.onnx.symbolic_opset8
    "addmm",
    "bmm",
    "empty",
    "empty_like",
    "flatten",
    "full",
    "full_like",
    "gt",
    "lt",
    "matmul",
    "mm",
    "ones",
    "ones_like",
    "prelu",
    "repeat",
    "zeros",
    "zeros_like",
    # torch.onnx.symbolic_opset9
    "abs",
    "acos",
    "adaptive_avg_pool1d",
    "adaptive_avg_pool2d",
    "adaptive_avg_pool3d",
    "adaptive_max_pool1d",
    "adaptive_max_pool2d",
    "adaptive_max_pool3d",
    "add",
    "addcmul",
    "addmm",
    "alias",
    "amax",
    "amin",
    "aminmax",
    "arange",
    "argmax",
    "argmin",
    "as_strided",
    "as_tensor",
    "asin",
    "atan",
    "atan2",
    "avg_pool1d",
    "avg_pool2d",
    "avg_pool3d",
    "baddbmm",
    "batch_norm",
    "bernoulli",
    "bitwise_not",
    "bitwise_or",
    "bmm",
    "broadcast_tensors",
    "broadcast_to",
    "bucketize",
    "cat",
    "cdist",
    "ceil",
    "clamp",
    "clamp_max",
    "clamp_min",
    "clone",
    "constant_pad_nd",
    "contiguous",
    "conv1d",
    "conv2d",
    "conv3d",
    "conv_tbc",
    "conv_transpose1d",
    "conv_transpose2d",
    "conv_transpose3d",
    "convert_element_type",
    "convolution",
    "cos",
    "cosine_similarity",
    "cross",
    "cumsum",
    "detach",
    "dim",
    "div",
    "dot",
    "dropout",
    "elu",
    "embedding",
    "embedding_bag",
    "empty",
    "empty_like",
    "eq",
    "erf",
    "exp",
    "expand",
    "expand_as",
    "eye",
    "fill",
    "flatten",
    "floor",
    "floor_divide",
    "floordiv",
    "frobenius_norm",
    "full",
    "full_like",
    "gather",
    "ge",
    "gelu",
    "get_pool_ceil_padding",
    "glu",
    "group_norm",
    "gru",
    "gt",
    "hann_window",
    "hardshrink",
    "hardsigmoid",
    "hardswish",
    "hardtanh",
    "index",
    "index_add",
    "index_copy",
    "index_fill",
    "index_put",
    "index_select",
    "instance_norm",
    "is_floating_point",
    "is_pinned",
    "isnan",
    "item",
    "kl_div",
    "layer_norm",
    "le",
    "leaky_relu",
    "lerp",
    "lift",
    "linalg_cross",
    "linalg_matrix_norm",
    "linalg_norm",
    "linalg_vector_norm",
    "linear",
    "linspace",
    "log",
    "log10",
    "log1p",
    "log2",
    "log_sigmoid",
    "log_softmax",
    "logical_and",
    "logical_not",
    "logical_or",
    "logical_xor",
    "logit",
    "logsumexp",
    "lstm",
    "lstm_cell",
    "lt",
    "masked_fill",
    "masked_fill_",
    "matmul",
    "max",
    "max_pool1d",
    "max_pool1d_with_indices",
    "max_pool2d",
    "max_pool2d_with_indices",
    "max_pool3d",
    "max_pool3d_with_indices",
    "maximum",
    "meshgrid",
    "min",
    "minimum",
    "mish",
    "mm",
    "movedim",
    "mse_loss",
    "mul",
    "multinomial",
    "mv",
    "narrow",
    "native_layer_norm",
    "ne",
    "neg",
    "new_empty",
    "new_full",
    "new_ones",
    "new_zeros",
    "nonzero",
    "nonzero_numpy",
    "noop_complex_operators",
    "norm",
    "numel",
    "numpy_T",
    "one_hot",
    "ones",
    "ones_like",
    "onnx_placeholder",
    "overload_by_arg_count",
    "pad",
    "pairwise_distance",
    "permute",
    "pixel_shuffle",
    "pixel_unshuffle",
    "pow",
    "prelu",
    "prim_constant",
    "prim_constant_chunk",
    "prim_constant_split",
    "prim_data",
    "prim_device",
    "prim_dtype",
    "prim_if",
    "prim_layout",
    "prim_list_construct",
    "prim_list_unpack",
    "prim_loop",
    "prim_max",
    "prim_min",
    "prim_shape",
    "prim_tolist",
    "prim_tuple_construct",
    "prim_type",
    "prim_unchecked_cast",
    "prim_uninitialized",
    "rand",
    "rand_like",
    "randint",
    "randint_like",
    "randn",
    "randn_like",
    "reciprocal",
    "reflection_pad",
    "relu",
    "relu6",
    "remainder",
    "repeat",
    "repeat_interleave",
    "replication_pad",
    "reshape",
    "reshape_as",
    "rnn_relu",
    "rnn_tanh",
    "roll",
    "rrelu",
    "rsqrt",
    "rsub",
    "scalar_tensor",
    "scatter",
    "scatter_add",
    "select",
    "selu",
    "sigmoid",
    "sign",
    "silu",
    "sin",
    "size",
    "slice",
    "softmax",
    "softplus",
    "softshrink",
    "sort",
    "split",
    "split_with_sizes",
    "sqrt",
    "square",
    "squeeze",
    "stack",
    "std",
    "std_mean",
    "sub",
    "t",
    "take",
    "tan",
    "tanh",
    "tanhshrink",
    "tensor",
    "threshold",
    "to",
    "topk",
    "transpose",
    "true_divide",
    "type_as",
    "unbind",
    "unfold",
    "unsafe_chunk",
    "unsafe_split",
    "unsafe_split_with_sizes",
    "unsqueeze",
    "unsupported_complex_operators",
    "unused",
    "upsample_bilinear2d",
    "upsample_linear1d",
    "upsample_nearest1d",
    "upsample_nearest2d",
    "upsample_nearest3d",
    "upsample_trilinear3d",
    "var",
    "var_mean",
    "view",
    "view_as",
    "where",
    "wrap_logical_op_with_cast_to",
    "wrap_logical_op_with_negation",
    "zero",
    "zeros",
    "zeros_like",
    # torch.onnx.utils
    "disable_apex_o2_state_dict_hook",
    "export",
    "export_to_pretty_string",
    "exporter_context",
    "is_in_onnx_export",
    "model_signature",
    "register_custom_op_symbolic",
    "select_model_mode_for_export",
    "setup_onnx_logging",
    "unconvertible_ops",
    "unpack_quantized_tensor",
    "warn_on_static_input_change",
    # torch.onnx.verification
    "check_export_model_diff",
    "verify",
    "verify_aten_graph",
    # torch.optim.optimizer
    "register_optimizer_step_post_hook",
    "register_optimizer_step_pre_hook",
    # torch.overrides
    "enable_reentrant_dispatch",
    # torch.package.analyze.find_first_use_of_broken_modules
    "find_first_use_of_broken_modules",
    # torch.package.analyze.is_from_package
    "is_from_package",
    # torch.package.analyze.trace_dependencies
    "trace_dependencies",
    # torch.profiler.itt
    "range",
    # torch.profiler.profiler
    "schedule",
    "supported_activities",
    "tensorboard_trace_handler",
    # torch.return_types
    "pytree_register_structseq",
    # torch.serialization
    "check_module_version_greater_or_equal",
    "default_restore_location",
    "load",
    "location_tag",
    "mkdtemp",
    "normalize_storage_type",
    "save",
    "storage_to_tensor_type",
    "validate_cuda_device",
    "validate_hpu_device",
    # torch.signal.windows.windows
    "bartlett",
    "blackman",
    "cosine",
    "exponential",
    "gaussian",
    "general_cosine",
    "general_hamming",
    "hamming",
    "hann",
    "kaiser",
    "nuttall",
    # torch.sparse.semi_structured
    "to_sparse_semi_structured",
    # torch.utils.backend_registration
    "generate_methods_for_privateuse1_backend",
    "rename_privateuse1_backend",
    # torch.utils.benchmark.examples.op_benchmark
    "assert_dicts_equal",
    # torch.utils.benchmark.op_fuzzers.spectral
    "power_range",
    # torch.utils.benchmark.utils.common
    "ordered_unique",
    "select_unit",
    "set_torch_threads",
    "trim_sigfig",
    "unit_to_english",
    # torch.utils.benchmark.utils.compare
    "optional_min",
    # torch.utils.benchmark.utils.compile
    "bench_all",
    "bench_loop",
    "benchmark_compile",
    # torch.utils.benchmark.utils.cpp_jit
    "compile_callgrind_template",
    "compile_timeit_template",
    "get_compat_bindings",
    # torch.utils.benchmark.utils.fuzzer
    "dtype_size",
    "prod",
    # torch.utils.benchmark.utils.timer
    "timer",
    # torch.utils.benchmark.utils.valgrind_wrapper.timer_interface
    "wrapper_singleton",
    # torch.utils.bundled_inputs
    "augment_many_model_functions_with_bundled_inputs",
    "augment_model_with_bundled_inputs",
    "bundle_inputs",
    "bundle_large_tensor",
    "bundle_randn",
    # torch.utils.checkpoint
    "check_backward_validity",
    "detach_variable",
    "get_device_states",
    "noop_context_fn",
    "set_checkpoint_early_stop",
    "set_device_states",
    # torch.utils.collect_env
    "check_release_file",
    "get_cachingallocator_config",
    "get_clang_version",
    "get_cmake_version",
    "get_conda_packages",
    "get_cpu_info",
    "get_cuda_module_loading_config",
    "get_cudnn_version",
    "get_env_info",
    "get_gcc_version",
    "get_gpu_info",
    "get_libc_version",
    "get_lsb_version",
    "get_mac_version",
    "get_nvidia_driver_version",
    "get_nvidia_smi",
    "get_os",
    "get_pip_packages",
    "get_platform",
    "get_pretty_env_info",
    "get_python_platform",
    "get_running_cuda_version",
    "get_windows_version",
    "is_xnnpack_available",
    "pretty_str",
    # torch.utils.cpp_backtrace
    "get_cpp_backtrace",
    # torch.utils.cpp_extension
    "check_compiler_is_gcc",
    "check_compiler_ok_for_platform",
    "get_cxx_compiler",
    "get_default_build_root",
    "library_paths",
    "remove_extension_h_precompiler_headers",
    # torch.utils.data.backward_compatibility
    "worker_init_fn",
    # torch.utils.data.datapipes.dataframe.dataframe_wrapper
    "concat",
    "create_dataframe",
    "get_columns",
    "get_df_wrapper",
    "get_item",
    "get_len",
    "is_column",
    "is_dataframe",
    "iterate",
    "set_df_wrapper",
    # torch.utils.data.datapipes.dataframe.dataframes
    "disable_capture",
    "get_val",
    # torch.utils.data.datapipes.gen_pyi
    "extract_class_name",
    "extract_method_name",
    "find_file_paths",
    "gen_from_template",
    "get_method_definitions",
    "materialize_lines",
    "parse_datapipe_file",
    "parse_datapipe_files",
    "process_signature",
    "split_outside_bracket",
    # torch.utils.data.datapipes.map.callable
    "default_fn",
    # torch.utils.data.datapipes.utils.common
    "get_file_binaries_from_pathnames",
    "get_file_pathnames_from_root",
    "match_masks",
    "validate_input_col",
    "validate_pathname_binary_tuple",
    # torch.utils.data.datapipes.utils.decoder
    "audiohandler",
    "basichandlers",
    "extension_extract_fn",
    "handle_extension",
    "imagehandler",
    "mathandler",
    "videohandler",
    # torch.utils.data.dataset
    "random_split",
    # torch.utils.data.graph
    "traverse",
    "traverse_dps",
    # torch.utils.data.graph_settings
    "apply_random_seed",
    "apply_sharding",
    "apply_shuffle_seed",
    "apply_shuffle_settings",
    "get_all_graph_pipes",
    # torch.utils.flop_counter
    "addmm_flop",
    "baddbmm_flop",
    "bmm_flop",
    "conv_backward_flop",
    "conv_flop",
    "conv_flop_count",
    "convert_num_with_suffix",
    "get_shape",
    "get_suffix_str",
    "mm_flop",
    "normalize_tuple",
    "register_flop_formula",
    "sdpa_backward_flop",
    "sdpa_backward_flop_count",
    "sdpa_flop",
    "sdpa_flop_count",
    "shape_wrapper",
    "transpose_shape",
    # torch.utils.hipify.hipify_python
    "add_dim3",
    "compute_stats",
    "extract_arguments",
    "file_add_header",
    "file_specific_replacement",
    "find_bracket_group",
    "find_closure_group",
    "find_parentheses_group",
    "fix_static_global_kernels",
    "get_hip_file_path",
    "hip_header_magic",
    "hipify",
    "is_caffe2_gpu_file",
    "is_cusparse_file",
    "is_out_of_p
```



## High-Level Overview

This file is part of the PyTorch framework located at `docs/docs/source`.

## Detailed Analysis

### Code Structure


*For complete code details, see the Original Source section above.*


## Architecture & Design

### Role in PyTorch Architecture

This file is located in `docs/docs/source`, which is part of the PyTorch project infrastructure.



## Dependencies

### Import Dependencies

*Dependency analysis not applicable for this file type.*


## Code Patterns & Idioms

### Common Patterns

- **Error Handling**: Includes exception handling
- **Neural Network**: Defines or uses PyTorch neural network components


## Performance Considerations

### Performance Notes

- This file appears to involve **GPU/parallel computing** capabilities.
- Implements or uses **caching** mechanisms.
- May involve **JIT compilation** or compilation optimizations.
- Contains **benchmarking** code or performance tests.

*Detailed performance analysis requires profiling and benchmarking.*


## Security & Safety

### Security Considerations

- **Serialization**: Uses pickle - be cautious with untrusted data

*Manual security review is recommended for production code.*


## Testing & Usage

### Testing

Test files for this module may be located in the `test/` directory.

### Usage Examples

*See the source code and related test files for usage examples.*


## Related Files

### Related Files

Files in the same folder (`docs/docs/source`):

- [`distributions.md_docs.md_docs.md`](./distributions.md_docs.md_docs.md)
- [`distributed.optim.md_docs.md_docs.md`](./distributed.optim.md_docs.md_docs.md)
- [`torch.compiler_dynamic_shapes.md_kw.md_docs.md`](./torch.compiler_dynamic_shapes.md_kw.md_docs.md)
- [`tensor_attributes.rst_docs.md_docs.md`](./tensor_attributes.rst_docs.md_docs.md)
- [`tensor_attributes.rst_kw.md_docs.md`](./tensor_attributes.rst_kw.md_docs.md)
- [`torch.compiler_dynamo_overview.md_docs.md_docs.md`](./torch.compiler_dynamo_overview.md_docs.md_docs.md)
- [`mtia.memory.md_kw.md_docs.md`](./mtia.memory.md_kw.md_docs.md)
- [`nn.attention.varlen.md_kw.md_docs.md`](./nn.attention.varlen.md_kw.md_docs.md)
- [`cpu.rst_kw.md_docs.md`](./cpu.rst_kw.md_docs.md)
- [`torch.compiler_faq.md_docs.md_docs.md`](./torch.compiler_faq.md_docs.md_docs.md)


## Cross-References

- **File Documentation**: `conf.py_docs.md_docs.md`
- **Keyword Index**: `conf.py_docs.md_kw.md`
- **Folder Index**: `index.md`
- **Folder Documentation**: `doc.md`

---

*Generated by PyTorch Repository Documentation System*
